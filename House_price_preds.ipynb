{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "18cc6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbea7f",
   "metadata": {},
   "source": [
    "##### Deep learning\n",
    "\n",
    "##### ANN with Pytorch\n",
    "\n",
    "\n",
    "##### Feature engineering\n",
    "Categorial --- Embedding layer\n",
    "Continous variable\n",
    "\n",
    "##### pythonic class to create NN  Feed forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1dfcee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('houseprice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "099dc3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c56f18e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c82a81a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 9 columns):\n",
      "MSSubClass     1460 non-null int64\n",
      "MSZoning       1460 non-null object\n",
      "LotFrontage    1201 non-null float64\n",
      "Street         1460 non-null object\n",
      "LotShape       1460 non-null object\n",
      "YearBuilt      1460 non-null int64\n",
      "1stFlrSF       1460 non-null int64\n",
      "2ndFlrSF       1460 non-null int64\n",
      "SalePrice      1460 non-null int64\n",
      "dtypes: float64(1), int64(5), object(3)\n",
      "memory usage: 102.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df[['MSSubClass','MSZoning','LotFrontage', 'Street','LotShape','YearBuilt', '1stFlrSF','2ndFlrSF','SalePrice']]\n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f70ae57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name MSSubClass has 15 unique values\n",
      "Column name MSZoning has 5 unique values\n",
      "Column name LotFrontage has 111 unique values\n",
      "Column name Street has 2 unique values\n",
      "Column name LotShape has 4 unique values\n",
      "Column name YearBuilt has 112 unique values\n",
      "Column name 1stFlrSF has 753 unique values\n",
      "Column name 2ndFlrSF has 417 unique values\n",
      "Column name SalePrice has 663 unique values\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(\"Column name {} has {} unique values\".format(i,len(df[i].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2ea6b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yearnow = datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b339249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total Years'] = yearnow - df['YearBuilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a85ab75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     21\n",
       "1     48\n",
       "2     23\n",
       "3    109\n",
       "4     24\n",
       "Name: Total Years, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Total Years'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a97cd9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('YearBuilt',axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "61f43679",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating categorical features\n",
    "\n",
    "cat_features = ['MSSubClass', 'MSZoning','Street','LotShape']\n",
    "\n",
    "out_features = ['SalePrice']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "85c49c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 5, ..., 6, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lbl_encoders ={}\n",
    "\n",
    "lbl_encoders['MSSubClass']   =  LabelEncoder()\n",
    "lbl_encoders['MSSubClass'].fit_transform(df['MSSubClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18b583fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSSubClass': LabelEncoder()}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "722523e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lbl_encoders ={}\n",
    "\n",
    "for feature in cat_features:\n",
    "    lbl_encoders[feature]   =  LabelEncoder()\n",
    "    df[feature] = lbl_encoders[feature].fit_transform(df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9332fdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  MSZoning  LotFrontage  Street  LotShape  1stFlrSF  2ndFlrSF  \\\n",
       "0           5         3         65.0       1         3       856       854   \n",
       "1           0         3         80.0       1         3      1262         0   \n",
       "\n",
       "   SalePrice  Total Years  \n",
       "0     208500           21  \n",
       "1     181500           48  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d32ed160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [5, 3, 1, 0],\n",
       "       ...,\n",
       "       [6, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [0, 3, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature = np.stack([df['MSSubClass'],df['MSZoning'],df['Street'],df['LotShape']],1 )\n",
    "cat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f78d47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "cat_feature = torch.tensor(cat_feature,dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bc3f1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Treating continous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d76723bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = []\n",
    "\n",
    "for i in df.columns:\n",
    "    if i not in ['LotFrontage', 'LotArea','1stFlrSF', '2ndFlrSF',  'Total Years']:\n",
    "        pass\n",
    "    else:\n",
    "        cont_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ecdf46b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', '1stFlrSF', '2ndFlrSF', 'Total Years']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "45d58c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  65.,  856.,  854.,   21.],\n",
       "        [  80., 1262.,    0.,   48.],\n",
       "        [  68.,  920.,  866.,   23.],\n",
       "        ...,\n",
       "        [  66., 1188., 1152.,   83.],\n",
       "        [  68., 1078.,    0.,   74.],\n",
       "        [  75., 1256.,    0.,   59.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## stacking cont variable to a tensor\n",
    "\n",
    "cont_variable =  np.stack([df[i].values for i in cont_features],axis = 1)\n",
    "cont_variable = torch.tensor(cont_variable,dtype=torch.float)\n",
    "cont_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5df82b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_variable.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4d8319e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(df['SalePrice'].values,dtype=torch.float).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e24242e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[208500.],\n",
       "        [181500.],\n",
       "        [223500.],\n",
       "        ...,\n",
       "        [266500.],\n",
       "        [142125.],\n",
       "        [147500.]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8f8b5440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 9 columns):\n",
      "MSSubClass     1460 non-null int64\n",
      "MSZoning       1460 non-null int32\n",
      "LotFrontage    1201 non-null float64\n",
      "Street         1460 non-null int32\n",
      "LotShape       1460 non-null int32\n",
      "1stFlrSF       1460 non-null int64\n",
      "2ndFlrSF       1460 non-null int64\n",
      "SalePrice      1460 non-null int64\n",
      "Total Years    1460 non-null int64\n",
      "dtypes: float64(1), int32(3), int64(5)\n",
      "memory usage: 85.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "71b0895c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1460, 4]), torch.Size([1460, 4]), torch.Size([1460, 1]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature.shape,cont_variable.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6e928fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####   EMBEDDING SIZE FOR CATEGORICAL VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7c63e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dims = [len(df[cols].unique()) for cols in ['MSSubClass', 'MSZoning', 'Street', 'LotShape']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6a25c568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 2, 4]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580d5df",
   "metadata": {},
   "source": [
    "#### THUMPS RULE TO SET OUTPUT DIMENSION BASED ON INPUT DIM\n",
    "\n",
    "##### input feature numbers + 1 and // 2     not more tthan 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "81f8ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = [(x, min(50, (x+1)// 2)) for x in cat_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f7db59ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 8), (5, 3), (2, 1), (4, 2)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2d127575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(15, 8)\n",
       "  (1): Embedding(5, 3)\n",
       "  (2): Embedding(2, 1)\n",
       "  (3): Embedding(4, 2)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "embed_representation = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "embed_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5b1effc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7842d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_feature = cat_feature[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "628474c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "34f4b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',500)\n",
    "embedding_val=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1eb10dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,e in enumerate(embed_representation):\n",
    "    embedding_val.append(e(cat_feature[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1ba9ac7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.2183, -0.1019, -0.5225,  ...,  0.1635,  1.8438, -0.2426],\n",
       "         [ 0.2639,  1.0908, -0.3652,  ..., -0.3927,  2.3561, -1.0678],\n",
       "         [ 1.2183, -0.1019, -0.5225,  ...,  0.1635,  1.8438, -0.2426],\n",
       "         ...,\n",
       "         [ 0.4322, -0.3227,  1.0355,  ..., -1.4795,  0.1584,  0.0389],\n",
       "         [ 0.2639,  1.0908, -0.3652,  ..., -0.3927,  2.3561, -1.0678],\n",
       "         [ 0.2639,  1.0908, -0.3652,  ..., -0.3927,  2.3561, -1.0678]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.0659,  0.8124, -0.6910],\n",
       "         [-0.0659,  0.8124, -0.6910],\n",
       "         [-0.0659,  0.8124, -0.6910],\n",
       "         ...,\n",
       "         [-0.0659,  0.8124, -0.6910],\n",
       "         [-0.0659,  0.8124, -0.6910],\n",
       "         [-0.0659,  0.8124, -0.6910]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[0.8636],\n",
       "         [0.8636],\n",
       "         [0.8636],\n",
       "         ...,\n",
       "         [0.8636],\n",
       "         [0.8636],\n",
       "         [0.8636]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[ 1.3932, -1.0638],\n",
       "         [ 1.3932, -1.0638],\n",
       "         [-0.2545,  0.4102],\n",
       "         ...,\n",
       "         [ 1.3932, -1.0638],\n",
       "         [ 1.3932, -1.0638],\n",
       "         [ 1.3932, -1.0638]], grad_fn=<EmbeddingBackward0>)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c13e759c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2183, -0.1019, -0.5225,  ...,  0.8636,  1.3932, -1.0638],\n",
       "        [ 0.2639,  1.0908, -0.3652,  ...,  0.8636,  1.3932, -1.0638],\n",
       "        [ 1.2183, -0.1019, -0.5225,  ...,  0.8636, -0.2545,  0.4102],\n",
       "        ...,\n",
       "        [ 0.4322, -0.3227,  1.0355,  ...,  0.8636,  1.3932, -1.0638],\n",
       "        [ 0.2639,  1.0908, -0.3652,  ...,  0.8636,  1.3932, -1.0638],\n",
       "        [ 0.2639,  1.0908, -0.3652,  ...,  0.8636,  1.3932, -1.0638]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.cat(embedding_val,1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "265db7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cf3d83c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0304, -0.1699, -0.8708,  ...,  1.4393,  0.0000, -1.7731],\n",
       "        [ 0.0000,  1.8179, -0.6087,  ...,  1.4393,  0.0000, -1.7731],\n",
       "        [ 0.0000, -0.1699, -0.8708,  ...,  0.0000, -0.4241,  0.6836],\n",
       "        ...,\n",
       "        [ 0.7203, -0.5379,  0.0000,  ...,  1.4393,  0.0000, -1.7731],\n",
       "        [ 0.0000,  1.8179, -0.6087,  ...,  1.4393,  2.3221, -0.0000],\n",
       "        [ 0.4398,  1.8179, -0.6087,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embed = dropout(z)\n",
    "final_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad734fd2",
   "metadata": {},
   "source": [
    "####  Creatinng the feed forwwardr NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d7be083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, n_cont, out_sz, layers, p = 0.5):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((out for inp,out in embedding_dim))\n",
    "        n_in = n_emb + n_cont\n",
    "        \n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i))\n",
    "            layerlist.append(nn.ReLU(inplace = True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x = torch.cat(embeddings,1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x,x_cont],1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "91a8d9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ec78d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "model = FeedForwardNN(embedding_dim,len(cont_features),1,[100,50],p = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c8952fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c1e7cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defining loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5e7374d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ceedc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "\n",
    "optimizer= torch.optim.Adam(model.parameters(),lr= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ead34638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 9)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9295fc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  65.,  856.,  854.,   21.],\n",
       "        [  80., 1262.,    0.,   48.],\n",
       "        [  68.,  920.,  866.,   23.],\n",
       "        ...,\n",
       "        [  66., 1188., 1152.,   83.],\n",
       "        [  68., 1078.,    0.,   74.],\n",
       "        [  75., 1256.,    0.,   59.]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e4b3e872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1460, 4])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_variable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bed962ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1460\n",
    "test_size = int(batch_size* 0.15)\n",
    "\n",
    "train_categorical = cat_feature[:batch_size-test_size]\n",
    "\n",
    "test_categorical = cat_feature[batch_size-test_size:batch_size]\n",
    "                                \n",
    "train_cont = cont_variable[:batch_size-test_size]\n",
    "\n",
    "test_cont = cont_variable[batch_size-test_size:batch_size]\n",
    "\n",
    "\n",
    "y_train = y[:batch_size-test_size]\n",
    "\n",
    "y_test = y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "02875061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 219, 1241, 219, 1241, 1241)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_categorical),len(test_cont),len(train_cont),len(y_test),len(y_train),len(train_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7a64af44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ac14f172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs num: 1 and the loss: nan\n",
      "Epochs num: 11 and the loss: nan\n",
      "Epochs num: 21 and the loss: nan\n",
      "Epochs num: 31 and the loss: nan\n",
      "Epochs num: 41 and the loss: nan\n",
      "Epochs num: 51 and the loss: nan\n",
      "Epochs num: 61 and the loss: nan\n",
      "Epochs num: 71 and the loss: nan\n",
      "Epochs num: 81 and the loss: nan\n",
      "Epochs num: 91 and the loss: nan\n",
      "Epochs num: 101 and the loss: nan\n",
      "Epochs num: 111 and the loss: nan\n",
      "Epochs num: 121 and the loss: nan\n",
      "Epochs num: 131 and the loss: nan\n",
      "Epochs num: 141 and the loss: nan\n",
      "Epochs num: 151 and the loss: nan\n",
      "Epochs num: 161 and the loss: nan\n",
      "Epochs num: 171 and the loss: nan\n",
      "Epochs num: 181 and the loss: nan\n",
      "Epochs num: 191 and the loss: nan\n",
      "Epochs num: 201 and the loss: nan\n",
      "Epochs num: 211 and the loss: nan\n",
      "Epochs num: 221 and the loss: nan\n",
      "Epochs num: 231 and the loss: nan\n",
      "Epochs num: 241 and the loss: nan\n",
      "Epochs num: 251 and the loss: nan\n",
      "Epochs num: 261 and the loss: nan\n",
      "Epochs num: 271 and the loss: nan\n",
      "Epochs num: 281 and the loss: nan\n",
      "Epochs num: 291 and the loss: nan\n",
      "Epochs num: 301 and the loss: nan\n",
      "Epochs num: 311 and the loss: nan\n",
      "Epochs num: 321 and the loss: nan\n",
      "Epochs num: 331 and the loss: nan\n",
      "Epochs num: 341 and the loss: nan\n",
      "Epochs num: 351 and the loss: nan\n",
      "Epochs num: 361 and the loss: nan\n",
      "Epochs num: 371 and the loss: nan\n",
      "Epochs num: 381 and the loss: nan\n",
      "Epochs num: 391 and the loss: nan\n",
      "Epochs num: 401 and the loss: nan\n",
      "Epochs num: 411 and the loss: nan\n",
      "Epochs num: 421 and the loss: nan\n",
      "Epochs num: 431 and the loss: nan\n",
      "Epochs num: 441 and the loss: nan\n",
      "Epochs num: 451 and the loss: nan\n",
      "Epochs num: 461 and the loss: nan\n",
      "Epochs num: 471 and the loss: nan\n",
      "Epochs num: 481 and the loss: nan\n",
      "Epochs num: 491 and the loss: nan\n",
      "Epochs num: 501 and the loss: nan\n",
      "Epochs num: 511 and the loss: nan\n",
      "Epochs num: 521 and the loss: nan\n",
      "Epochs num: 531 and the loss: nan\n",
      "Epochs num: 541 and the loss: nan\n",
      "Epochs num: 551 and the loss: nan\n",
      "Epochs num: 561 and the loss: nan\n",
      "Epochs num: 571 and the loss: nan\n",
      "Epochs num: 581 and the loss: nan\n",
      "Epochs num: 591 and the loss: nan\n",
      "Epochs num: 601 and the loss: nan\n",
      "Epochs num: 611 and the loss: nan\n",
      "Epochs num: 621 and the loss: nan\n",
      "Epochs num: 631 and the loss: nan\n",
      "Epochs num: 641 and the loss: nan\n",
      "Epochs num: 651 and the loss: nan\n",
      "Epochs num: 661 and the loss: nan\n",
      "Epochs num: 671 and the loss: nan\n",
      "Epochs num: 681 and the loss: nan\n",
      "Epochs num: 691 and the loss: nan\n",
      "Epochs num: 701 and the loss: nan\n",
      "Epochs num: 711 and the loss: nan\n",
      "Epochs num: 721 and the loss: nan\n",
      "Epochs num: 731 and the loss: nan\n",
      "Epochs num: 741 and the loss: nan\n",
      "Epochs num: 751 and the loss: nan\n",
      "Epochs num: 761 and the loss: nan\n",
      "Epochs num: 771 and the loss: nan\n",
      "Epochs num: 781 and the loss: nan\n",
      "Epochs num: 791 and the loss: nan\n",
      "Epochs num: 801 and the loss: nan\n",
      "Epochs num: 811 and the loss: nan\n",
      "Epochs num: 821 and the loss: nan\n",
      "Epochs num: 831 and the loss: nan\n",
      "Epochs num: 841 and the loss: nan\n",
      "Epochs num: 851 and the loss: nan\n",
      "Epochs num: 861 and the loss: nan\n",
      "Epochs num: 871 and the loss: nan\n",
      "Epochs num: 881 and the loss: nan\n",
      "Epochs num: 891 and the loss: nan\n",
      "Epochs num: 901 and the loss: nan\n",
      "Epochs num: 911 and the loss: nan\n",
      "Epochs num: 921 and the loss: nan\n",
      "Epochs num: 931 and the loss: nan\n",
      "Epochs num: 941 and the loss: nan\n",
      "Epochs num: 951 and the loss: nan\n",
      "Epochs num: 961 and the loss: nan\n",
      "Epochs num: 971 and the loss: nan\n",
      "Epochs num: 981 and the loss: nan\n",
      "Epochs num: 991 and the loss: nan\n",
      "Epochs num: 1001 and the loss: nan\n",
      "Epochs num: 1011 and the loss: nan\n",
      "Epochs num: 1021 and the loss: nan\n",
      "Epochs num: 1031 and the loss: nan\n",
      "Epochs num: 1041 and the loss: nan\n",
      "Epochs num: 1051 and the loss: nan\n",
      "Epochs num: 1061 and the loss: nan\n",
      "Epochs num: 1071 and the loss: nan\n",
      "Epochs num: 1081 and the loss: nan\n",
      "Epochs num: 1091 and the loss: nan\n",
      "Epochs num: 1101 and the loss: nan\n",
      "Epochs num: 1111 and the loss: nan\n",
      "Epochs num: 1121 and the loss: nan\n",
      "Epochs num: 1131 and the loss: nan\n",
      "Epochs num: 1141 and the loss: nan\n",
      "Epochs num: 1151 and the loss: nan\n",
      "Epochs num: 1161 and the loss: nan\n",
      "Epochs num: 1171 and the loss: nan\n",
      "Epochs num: 1181 and the loss: nan\n",
      "Epochs num: 1191 and the loss: nan\n",
      "Epochs num: 1201 and the loss: nan\n",
      "Epochs num: 1211 and the loss: nan\n",
      "Epochs num: 1221 and the loss: nan\n",
      "Epochs num: 1231 and the loss: nan\n",
      "Epochs num: 1241 and the loss: nan\n",
      "Epochs num: 1251 and the loss: nan\n",
      "Epochs num: 1261 and the loss: nan\n",
      "Epochs num: 1271 and the loss: nan\n",
      "Epochs num: 1281 and the loss: nan\n",
      "Epochs num: 1291 and the loss: nan\n",
      "Epochs num: 1301 and the loss: nan\n",
      "Epochs num: 1311 and the loss: nan\n",
      "Epochs num: 1321 and the loss: nan\n",
      "Epochs num: 1331 and the loss: nan\n",
      "Epochs num: 1341 and the loss: nan\n",
      "Epochs num: 1351 and the loss: nan\n",
      "Epochs num: 1361 and the loss: nan\n",
      "Epochs num: 1371 and the loss: nan\n",
      "Epochs num: 1381 and the loss: nan\n",
      "Epochs num: 1391 and the loss: nan\n",
      "Epochs num: 1401 and the loss: nan\n",
      "Epochs num: 1411 and the loss: nan\n",
      "Epochs num: 1421 and the loss: nan\n",
      "Epochs num: 1431 and the loss: nan\n",
      "Epochs num: 1441 and the loss: nan\n",
      "Epochs num: 1451 and the loss: nan\n",
      "Epochs num: 1461 and the loss: nan\n",
      "Epochs num: 1471 and the loss: nan\n",
      "Epochs num: 1481 and the loss: nan\n",
      "Epochs num: 1491 and the loss: nan\n",
      "Epochs num: 1501 and the loss: nan\n",
      "Epochs num: 1511 and the loss: nan\n",
      "Epochs num: 1521 and the loss: nan\n",
      "Epochs num: 1531 and the loss: nan\n",
      "Epochs num: 1541 and the loss: nan\n",
      "Epochs num: 1551 and the loss: nan\n",
      "Epochs num: 1561 and the loss: nan\n",
      "Epochs num: 1571 and the loss: nan\n",
      "Epochs num: 1581 and the loss: nan\n",
      "Epochs num: 1591 and the loss: nan\n",
      "Epochs num: 1601 and the loss: nan\n",
      "Epochs num: 1611 and the loss: nan\n",
      "Epochs num: 1621 and the loss: nan\n",
      "Epochs num: 1631 and the loss: nan\n",
      "Epochs num: 1641 and the loss: nan\n",
      "Epochs num: 1651 and the loss: nan\n",
      "Epochs num: 1661 and the loss: nan\n",
      "Epochs num: 1671 and the loss: nan\n",
      "Epochs num: 1681 and the loss: nan\n",
      "Epochs num: 1691 and the loss: nan\n",
      "Epochs num: 1701 and the loss: nan\n",
      "Epochs num: 1711 and the loss: nan\n",
      "Epochs num: 1721 and the loss: nan\n",
      "Epochs num: 1731 and the loss: nan\n",
      "Epochs num: 1741 and the loss: nan\n",
      "Epochs num: 1751 and the loss: nan\n",
      "Epochs num: 1761 and the loss: nan\n",
      "Epochs num: 1771 and the loss: nan\n",
      "Epochs num: 1781 and the loss: nan\n",
      "Epochs num: 1791 and the loss: nan\n",
      "Epochs num: 1801 and the loss: nan\n",
      "Epochs num: 1811 and the loss: nan\n",
      "Epochs num: 1821 and the loss: nan\n",
      "Epochs num: 1831 and the loss: nan\n",
      "Epochs num: 1841 and the loss: nan\n",
      "Epochs num: 1851 and the loss: nan\n",
      "Epochs num: 1861 and the loss: nan\n",
      "Epochs num: 1871 and the loss: nan\n",
      "Epochs num: 1881 and the loss: nan\n",
      "Epochs num: 1891 and the loss: nan\n",
      "Epochs num: 1901 and the loss: nan\n",
      "Epochs num: 1911 and the loss: nan\n",
      "Epochs num: 1921 and the loss: nan\n",
      "Epochs num: 1931 and the loss: nan\n",
      "Epochs num: 1941 and the loss: nan\n",
      "Epochs num: 1951 and the loss: nan\n",
      "Epochs num: 1961 and the loss: nan\n",
      "Epochs num: 1971 and the loss: nan\n",
      "Epochs num: 1981 and the loss: nan\n",
      "Epochs num: 1991 and the loss: nan\n",
      "Epochs num: 2001 and the loss: nan\n",
      "Epochs num: 2011 and the loss: nan\n",
      "Epochs num: 2021 and the loss: nan\n",
      "Epochs num: 2031 and the loss: nan\n",
      "Epochs num: 2041 and the loss: nan\n",
      "Epochs num: 2051 and the loss: nan\n",
      "Epochs num: 2061 and the loss: nan\n",
      "Epochs num: 2071 and the loss: nan\n",
      "Epochs num: 2081 and the loss: nan\n",
      "Epochs num: 2091 and the loss: nan\n",
      "Epochs num: 2101 and the loss: nan\n",
      "Epochs num: 2111 and the loss: nan\n",
      "Epochs num: 2121 and the loss: nan\n",
      "Epochs num: 2131 and the loss: nan\n",
      "Epochs num: 2141 and the loss: nan\n",
      "Epochs num: 2151 and the loss: nan\n",
      "Epochs num: 2161 and the loss: nan\n",
      "Epochs num: 2171 and the loss: nan\n",
      "Epochs num: 2181 and the loss: nan\n",
      "Epochs num: 2191 and the loss: nan\n",
      "Epochs num: 2201 and the loss: nan\n",
      "Epochs num: 2211 and the loss: nan\n",
      "Epochs num: 2221 and the loss: nan\n",
      "Epochs num: 2231 and the loss: nan\n",
      "Epochs num: 2241 and the loss: nan\n",
      "Epochs num: 2251 and the loss: nan\n",
      "Epochs num: 2261 and the loss: nan\n",
      "Epochs num: 2271 and the loss: nan\n",
      "Epochs num: 2281 and the loss: nan\n",
      "Epochs num: 2291 and the loss: nan\n",
      "Epochs num: 2301 and the loss: nan\n",
      "Epochs num: 2311 and the loss: nan\n",
      "Epochs num: 2321 and the loss: nan\n",
      "Epochs num: 2331 and the loss: nan\n",
      "Epochs num: 2341 and the loss: nan\n",
      "Epochs num: 2351 and the loss: nan\n",
      "Epochs num: 2361 and the loss: nan\n",
      "Epochs num: 2371 and the loss: nan\n",
      "Epochs num: 2381 and the loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs num: 2391 and the loss: nan\n",
      "Epochs num: 2401 and the loss: nan\n",
      "Epochs num: 2411 and the loss: nan\n",
      "Epochs num: 2421 and the loss: nan\n",
      "Epochs num: 2431 and the loss: nan\n",
      "Epochs num: 2441 and the loss: nan\n",
      "Epochs num: 2451 and the loss: nan\n",
      "Epochs num: 2461 and the loss: nan\n",
      "Epochs num: 2471 and the loss: nan\n",
      "Epochs num: 2481 and the loss: nan\n",
      "Epochs num: 2491 and the loss: nan\n",
      "Epochs num: 2501 and the loss: nan\n",
      "Epochs num: 2511 and the loss: nan\n",
      "Epochs num: 2521 and the loss: nan\n",
      "Epochs num: 2531 and the loss: nan\n",
      "Epochs num: 2541 and the loss: nan\n",
      "Epochs num: 2551 and the loss: nan\n",
      "Epochs num: 2561 and the loss: nan\n",
      "Epochs num: 2571 and the loss: nan\n",
      "Epochs num: 2581 and the loss: nan\n",
      "Epochs num: 2591 and the loss: nan\n",
      "Epochs num: 2601 and the loss: nan\n",
      "Epochs num: 2611 and the loss: nan\n",
      "Epochs num: 2621 and the loss: nan\n",
      "Epochs num: 2631 and the loss: nan\n",
      "Epochs num: 2641 and the loss: nan\n",
      "Epochs num: 2651 and the loss: nan\n",
      "Epochs num: 2661 and the loss: nan\n",
      "Epochs num: 2671 and the loss: nan\n",
      "Epochs num: 2681 and the loss: nan\n",
      "Epochs num: 2691 and the loss: nan\n",
      "Epochs num: 2701 and the loss: nan\n",
      "Epochs num: 2711 and the loss: nan\n",
      "Epochs num: 2721 and the loss: nan\n",
      "Epochs num: 2731 and the loss: nan\n",
      "Epochs num: 2741 and the loss: nan\n",
      "Epochs num: 2751 and the loss: nan\n",
      "Epochs num: 2761 and the loss: nan\n",
      "Epochs num: 2771 and the loss: nan\n",
      "Epochs num: 2781 and the loss: nan\n",
      "Epochs num: 2791 and the loss: nan\n",
      "Epochs num: 2801 and the loss: nan\n",
      "Epochs num: 2811 and the loss: nan\n",
      "Epochs num: 2821 and the loss: nan\n",
      "Epochs num: 2831 and the loss: nan\n",
      "Epochs num: 2841 and the loss: nan\n",
      "Epochs num: 2851 and the loss: nan\n",
      "Epochs num: 2861 and the loss: nan\n",
      "Epochs num: 2871 and the loss: nan\n",
      "Epochs num: 2881 and the loss: nan\n",
      "Epochs num: 2891 and the loss: nan\n",
      "Epochs num: 2901 and the loss: nan\n",
      "Epochs num: 2911 and the loss: nan\n",
      "Epochs num: 2921 and the loss: nan\n",
      "Epochs num: 2931 and the loss: nan\n",
      "Epochs num: 2941 and the loss: nan\n",
      "Epochs num: 2951 and the loss: nan\n",
      "Epochs num: 2961 and the loss: nan\n",
      "Epochs num: 2971 and the loss: nan\n",
      "Epochs num: 2981 and the loss: nan\n",
      "Epochs num: 2991 and the loss: nan\n",
      "Epochs num: 3001 and the loss: nan\n",
      "Epochs num: 3011 and the loss: nan\n",
      "Epochs num: 3021 and the loss: nan\n",
      "Epochs num: 3031 and the loss: nan\n",
      "Epochs num: 3041 and the loss: nan\n",
      "Epochs num: 3051 and the loss: nan\n",
      "Epochs num: 3061 and the loss: nan\n",
      "Epochs num: 3071 and the loss: nan\n",
      "Epochs num: 3081 and the loss: nan\n",
      "Epochs num: 3091 and the loss: nan\n",
      "Epochs num: 3101 and the loss: nan\n",
      "Epochs num: 3111 and the loss: nan\n",
      "Epochs num: 3121 and the loss: nan\n",
      "Epochs num: 3131 and the loss: nan\n",
      "Epochs num: 3141 and the loss: nan\n",
      "Epochs num: 3151 and the loss: nan\n",
      "Epochs num: 3161 and the loss: nan\n",
      "Epochs num: 3171 and the loss: nan\n",
      "Epochs num: 3181 and the loss: nan\n",
      "Epochs num: 3191 and the loss: nan\n",
      "Epochs num: 3201 and the loss: nan\n",
      "Epochs num: 3211 and the loss: nan\n",
      "Epochs num: 3221 and the loss: nan\n",
      "Epochs num: 3231 and the loss: nan\n",
      "Epochs num: 3241 and the loss: nan\n",
      "Epochs num: 3251 and the loss: nan\n",
      "Epochs num: 3261 and the loss: nan\n",
      "Epochs num: 3271 and the loss: nan\n",
      "Epochs num: 3281 and the loss: nan\n",
      "Epochs num: 3291 and the loss: nan\n",
      "Epochs num: 3301 and the loss: nan\n",
      "Epochs num: 3311 and the loss: nan\n",
      "Epochs num: 3321 and the loss: nan\n",
      "Epochs num: 3331 and the loss: nan\n",
      "Epochs num: 3341 and the loss: nan\n",
      "Epochs num: 3351 and the loss: nan\n",
      "Epochs num: 3361 and the loss: nan\n",
      "Epochs num: 3371 and the loss: nan\n",
      "Epochs num: 3381 and the loss: nan\n",
      "Epochs num: 3391 and the loss: nan\n",
      "Epochs num: 3401 and the loss: nan\n",
      "Epochs num: 3411 and the loss: nan\n",
      "Epochs num: 3421 and the loss: nan\n",
      "Epochs num: 3431 and the loss: nan\n",
      "Epochs num: 3441 and the loss: nan\n",
      "Epochs num: 3451 and the loss: nan\n",
      "Epochs num: 3461 and the loss: nan\n",
      "Epochs num: 3471 and the loss: nan\n",
      "Epochs num: 3481 and the loss: nan\n",
      "Epochs num: 3491 and the loss: nan\n",
      "Epochs num: 3501 and the loss: nan\n",
      "Epochs num: 3511 and the loss: nan\n",
      "Epochs num: 3521 and the loss: nan\n",
      "Epochs num: 3531 and the loss: nan\n",
      "Epochs num: 3541 and the loss: nan\n",
      "Epochs num: 3551 and the loss: nan\n",
      "Epochs num: 3561 and the loss: nan\n",
      "Epochs num: 3571 and the loss: nan\n",
      "Epochs num: 3581 and the loss: nan\n",
      "Epochs num: 3591 and the loss: nan\n",
      "Epochs num: 3601 and the loss: nan\n",
      "Epochs num: 3611 and the loss: nan\n",
      "Epochs num: 3621 and the loss: nan\n",
      "Epochs num: 3631 and the loss: nan\n",
      "Epochs num: 3641 and the loss: nan\n",
      "Epochs num: 3651 and the loss: nan\n",
      "Epochs num: 3661 and the loss: nan\n",
      "Epochs num: 3671 and the loss: nan\n",
      "Epochs num: 3681 and the loss: nan\n",
      "Epochs num: 3691 and the loss: nan\n",
      "Epochs num: 3701 and the loss: nan\n",
      "Epochs num: 3711 and the loss: nan\n",
      "Epochs num: 3721 and the loss: nan\n",
      "Epochs num: 3731 and the loss: nan\n",
      "Epochs num: 3741 and the loss: nan\n",
      "Epochs num: 3751 and the loss: nan\n",
      "Epochs num: 3761 and the loss: nan\n",
      "Epochs num: 3771 and the loss: nan\n",
      "Epochs num: 3781 and the loss: nan\n",
      "Epochs num: 3791 and the loss: nan\n",
      "Epochs num: 3801 and the loss: nan\n",
      "Epochs num: 3811 and the loss: nan\n",
      "Epochs num: 3821 and the loss: nan\n",
      "Epochs num: 3831 and the loss: nan\n",
      "Epochs num: 3841 and the loss: nan\n",
      "Epochs num: 3851 and the loss: nan\n",
      "Epochs num: 3861 and the loss: nan\n",
      "Epochs num: 3871 and the loss: nan\n",
      "Epochs num: 3881 and the loss: nan\n",
      "Epochs num: 3891 and the loss: nan\n",
      "Epochs num: 3901 and the loss: nan\n",
      "Epochs num: 3911 and the loss: nan\n",
      "Epochs num: 3921 and the loss: nan\n",
      "Epochs num: 3931 and the loss: nan\n",
      "Epochs num: 3941 and the loss: nan\n",
      "Epochs num: 3951 and the loss: nan\n",
      "Epochs num: 3961 and the loss: nan\n",
      "Epochs num: 3971 and the loss: nan\n",
      "Epochs num: 3981 and the loss: nan\n",
      "Epochs num: 3991 and the loss: nan\n",
      "Epochs num: 4001 and the loss: nan\n",
      "Epochs num: 4011 and the loss: nan\n",
      "Epochs num: 4021 and the loss: nan\n",
      "Epochs num: 4031 and the loss: nan\n",
      "Epochs num: 4041 and the loss: nan\n",
      "Epochs num: 4051 and the loss: nan\n",
      "Epochs num: 4061 and the loss: nan\n",
      "Epochs num: 4071 and the loss: nan\n",
      "Epochs num: 4081 and the loss: nan\n",
      "Epochs num: 4091 and the loss: nan\n",
      "Epochs num: 4101 and the loss: nan\n",
      "Epochs num: 4111 and the loss: nan\n",
      "Epochs num: 4121 and the loss: nan\n",
      "Epochs num: 4131 and the loss: nan\n",
      "Epochs num: 4141 and the loss: nan\n",
      "Epochs num: 4151 and the loss: nan\n",
      "Epochs num: 4161 and the loss: nan\n",
      "Epochs num: 4171 and the loss: nan\n",
      "Epochs num: 4181 and the loss: nan\n",
      "Epochs num: 4191 and the loss: nan\n",
      "Epochs num: 4201 and the loss: nan\n",
      "Epochs num: 4211 and the loss: nan\n",
      "Epochs num: 4221 and the loss: nan\n",
      "Epochs num: 4231 and the loss: nan\n",
      "Epochs num: 4241 and the loss: nan\n",
      "Epochs num: 4251 and the loss: nan\n",
      "Epochs num: 4261 and the loss: nan\n",
      "Epochs num: 4271 and the loss: nan\n",
      "Epochs num: 4281 and the loss: nan\n",
      "Epochs num: 4291 and the loss: nan\n",
      "Epochs num: 4301 and the loss: nan\n",
      "Epochs num: 4311 and the loss: nan\n",
      "Epochs num: 4321 and the loss: nan\n",
      "Epochs num: 4331 and the loss: nan\n",
      "Epochs num: 4341 and the loss: nan\n",
      "Epochs num: 4351 and the loss: nan\n",
      "Epochs num: 4361 and the loss: nan\n",
      "Epochs num: 4371 and the loss: nan\n",
      "Epochs num: 4381 and the loss: nan\n",
      "Epochs num: 4391 and the loss: nan\n",
      "Epochs num: 4401 and the loss: nan\n",
      "Epochs num: 4411 and the loss: nan\n",
      "Epochs num: 4421 and the loss: nan\n",
      "Epochs num: 4431 and the loss: nan\n",
      "Epochs num: 4441 and the loss: nan\n",
      "Epochs num: 4451 and the loss: nan\n",
      "Epochs num: 4461 and the loss: nan\n",
      "Epochs num: 4471 and the loss: nan\n",
      "Epochs num: 4481 and the loss: nan\n",
      "Epochs num: 4491 and the loss: nan\n",
      "Epochs num: 4501 and the loss: nan\n",
      "Epochs num: 4511 and the loss: nan\n",
      "Epochs num: 4521 and the loss: nan\n",
      "Epochs num: 4531 and the loss: nan\n",
      "Epochs num: 4541 and the loss: nan\n",
      "Epochs num: 4551 and the loss: nan\n",
      "Epochs num: 4561 and the loss: nan\n",
      "Epochs num: 4571 and the loss: nan\n",
      "Epochs num: 4581 and the loss: nan\n",
      "Epochs num: 4591 and the loss: nan\n",
      "Epochs num: 4601 and the loss: nan\n",
      "Epochs num: 4611 and the loss: nan\n",
      "Epochs num: 4621 and the loss: nan\n",
      "Epochs num: 4631 and the loss: nan\n",
      "Epochs num: 4641 and the loss: nan\n",
      "Epochs num: 4651 and the loss: nan\n",
      "Epochs num: 4661 and the loss: nan\n",
      "Epochs num: 4671 and the loss: nan\n",
      "Epochs num: 4681 and the loss: nan\n",
      "Epochs num: 4691 and the loss: nan\n",
      "Epochs num: 4701 and the loss: nan\n",
      "Epochs num: 4711 and the loss: nan\n",
      "Epochs num: 4721 and the loss: nan\n",
      "Epochs num: 4731 and the loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs num: 4741 and the loss: nan\n",
      "Epochs num: 4751 and the loss: nan\n",
      "Epochs num: 4761 and the loss: nan\n",
      "Epochs num: 4771 and the loss: nan\n",
      "Epochs num: 4781 and the loss: nan\n",
      "Epochs num: 4791 and the loss: nan\n",
      "Epochs num: 4801 and the loss: nan\n",
      "Epochs num: 4811 and the loss: nan\n",
      "Epochs num: 4821 and the loss: nan\n",
      "Epochs num: 4831 and the loss: nan\n",
      "Epochs num: 4841 and the loss: nan\n",
      "Epochs num: 4851 and the loss: nan\n",
      "Epochs num: 4861 and the loss: nan\n",
      "Epochs num: 4871 and the loss: nan\n",
      "Epochs num: 4881 and the loss: nan\n",
      "Epochs num: 4891 and the loss: nan\n",
      "Epochs num: 4901 and the loss: nan\n",
      "Epochs num: 4911 and the loss: nan\n",
      "Epochs num: 4921 and the loss: nan\n",
      "Epochs num: 4931 and the loss: nan\n",
      "Epochs num: 4941 and the loss: nan\n",
      "Epochs num: 4951 and the loss: nan\n",
      "Epochs num: 4961 and the loss: nan\n",
      "Epochs num: 4971 and the loss: nan\n",
      "Epochs num: 4981 and the loss: nan\n",
      "Epochs num: 4991 and the loss: nan\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "\n",
    "final_losses = []\n",
    "for i in range(epochs):\n",
    "    \n",
    "    i += 1\n",
    "    y_pred = model(train_categorical,train_cont)\n",
    "    loss = torch.sqrt(loss_function(y_pred,y_train))\n",
    "    final_losses.append(loss)\n",
    "    if i%10 == 1:\n",
    "        \n",
    "        print(\"Epochs num: {} and the loss: {}\".format(i,loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360cd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2204680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 0],\n",
       "        [5, 3, 1, 3]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "262dd5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  65.,  856.,  854.,   21.],\n",
       "        [  80., 1262.,    0.,   48.],\n",
       "        [  68.,  920.,  866.,   23.],\n",
       "        ...,\n",
       "        [  63., 1141.,    0.,   19.],\n",
       "        [  64., 1484.,    0.,   18.],\n",
       "        [  65.,  884.,  884.,   21.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "89afe0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(epochs), final_losses)\n",
    "plt.ylabel('RMSE Loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8d1d3af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: nan\n"
     ]
    }
   ],
   "source": [
    "#### Validate the Test Data\n",
    "y_pred=\"\"\n",
    "with torch.no_grad():\n",
    "    y_pred=model(test_categorical,test_cont)\n",
    "    loss=torch.sqrt(loss_function(y_pred,y_test))\n",
    "print('RMSE: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1734f8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Prediction\n",
       "0           NaN\n",
       "1           NaN\n",
       "2           NaN\n",
       "3           NaN\n",
       "4           NaN\n",
       "5           NaN\n",
       "6           NaN\n",
       "7           NaN\n",
       "8           NaN\n",
       "9           NaN\n",
       "10          NaN\n",
       "11          NaN\n",
       "12          NaN\n",
       "13          NaN\n",
       "14          NaN\n",
       "15          NaN\n",
       "16          NaN\n",
       "17          NaN\n",
       "18          NaN\n",
       "19          NaN\n",
       "20          NaN\n",
       "21          NaN\n",
       "22          NaN\n",
       "23          NaN\n",
       "24          NaN\n",
       "25          NaN\n",
       "26          NaN\n",
       "27          NaN\n",
       "28          NaN\n",
       "29          NaN\n",
       "30          NaN\n",
       "31          NaN\n",
       "32          NaN\n",
       "33          NaN\n",
       "34          NaN\n",
       "35          NaN\n",
       "36          NaN\n",
       "37          NaN\n",
       "38          NaN\n",
       "39          NaN\n",
       "40          NaN\n",
       "41          NaN\n",
       "42          NaN\n",
       "43          NaN\n",
       "44          NaN\n",
       "45          NaN\n",
       "46          NaN\n",
       "47          NaN\n",
       "48          NaN\n",
       "49          NaN\n",
       "50          NaN\n",
       "51          NaN\n",
       "52          NaN\n",
       "53          NaN\n",
       "54          NaN\n",
       "55          NaN\n",
       "56          NaN\n",
       "57          NaN\n",
       "58          NaN\n",
       "59          NaN\n",
       "60          NaN\n",
       "61          NaN\n",
       "62          NaN\n",
       "63          NaN\n",
       "64          NaN\n",
       "65          NaN\n",
       "66          NaN\n",
       "67          NaN\n",
       "68          NaN\n",
       "69          NaN\n",
       "70          NaN\n",
       "71          NaN\n",
       "72          NaN\n",
       "73          NaN\n",
       "74          NaN\n",
       "75          NaN\n",
       "76          NaN\n",
       "77          NaN\n",
       "78          NaN\n",
       "79          NaN\n",
       "80          NaN\n",
       "81          NaN\n",
       "82          NaN\n",
       "83          NaN\n",
       "84          NaN\n",
       "85          NaN\n",
       "86          NaN\n",
       "87          NaN\n",
       "88          NaN\n",
       "89          NaN\n",
       "90          NaN\n",
       "91          NaN\n",
       "92          NaN\n",
       "93          NaN\n",
       "94          NaN\n",
       "95          NaN\n",
       "96          NaN\n",
       "97          NaN\n",
       "98          NaN\n",
       "99          NaN\n",
       "100         NaN\n",
       "101         NaN\n",
       "102         NaN\n",
       "103         NaN\n",
       "104         NaN\n",
       "105         NaN\n",
       "106         NaN\n",
       "107         NaN\n",
       "108         NaN\n",
       "109         NaN\n",
       "110         NaN\n",
       "111         NaN\n",
       "112         NaN\n",
       "113         NaN\n",
       "114         NaN\n",
       "115         NaN\n",
       "116         NaN\n",
       "117         NaN\n",
       "118         NaN\n",
       "119         NaN\n",
       "120         NaN\n",
       "121         NaN\n",
       "122         NaN\n",
       "123         NaN\n",
       "124         NaN\n",
       "125         NaN\n",
       "126         NaN\n",
       "127         NaN\n",
       "128         NaN\n",
       "129         NaN\n",
       "130         NaN\n",
       "131         NaN\n",
       "132         NaN\n",
       "133         NaN\n",
       "134         NaN\n",
       "135         NaN\n",
       "136         NaN\n",
       "137         NaN\n",
       "138         NaN\n",
       "139         NaN\n",
       "140         NaN\n",
       "141         NaN\n",
       "142         NaN\n",
       "143         NaN\n",
       "144         NaN\n",
       "145         NaN\n",
       "146         NaN\n",
       "147         NaN\n",
       "148         NaN\n",
       "149         NaN\n",
       "150         NaN\n",
       "151         NaN\n",
       "152         NaN\n",
       "153         NaN\n",
       "154         NaN\n",
       "155         NaN\n",
       "156         NaN\n",
       "157         NaN\n",
       "158         NaN\n",
       "159         NaN\n",
       "160         NaN\n",
       "161         NaN\n",
       "162         NaN\n",
       "163         NaN\n",
       "164         NaN\n",
       "165         NaN\n",
       "166         NaN\n",
       "167         NaN\n",
       "168         NaN\n",
       "169         NaN\n",
       "170         NaN\n",
       "171         NaN\n",
       "172         NaN\n",
       "173         NaN\n",
       "174         NaN\n",
       "175         NaN\n",
       "176         NaN\n",
       "177         NaN\n",
       "178         NaN\n",
       "179         NaN\n",
       "180         NaN\n",
       "181         NaN\n",
       "182         NaN\n",
       "183         NaN\n",
       "184         NaN\n",
       "185         NaN\n",
       "186         NaN\n",
       "187         NaN\n",
       "188         NaN\n",
       "189         NaN\n",
       "190         NaN\n",
       "191         NaN\n",
       "192         NaN\n",
       "193         NaN\n",
       "194         NaN\n",
       "195         NaN\n",
       "196         NaN\n",
       "197         NaN\n",
       "198         NaN\n",
       "199         NaN\n",
       "200         NaN\n",
       "201         NaN\n",
       "202         NaN\n",
       "203         NaN\n",
       "204         NaN\n",
       "205         NaN\n",
       "206         NaN\n",
       "207         NaN\n",
       "208         NaN\n",
       "209         NaN\n",
       "210         NaN\n",
       "211         NaN\n",
       "212         NaN\n",
       "213         NaN\n",
       "214         NaN\n",
       "215         NaN\n",
       "216         NaN\n",
       "217         NaN\n",
       "218         NaN"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_verify=pd.DataFrame(y_test.tolist(),columns=[\"Test\"])\n",
    "data_predicted=pd.DataFrame(y_pred.tolist(),columns=[\"Prediction\"])\n",
    "data_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "604c4c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>248328.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>465000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test  Prediction  Difference\n",
       "0  248328.0         NaN         NaN\n",
       "1  170000.0         NaN         NaN\n",
       "2  465000.0         NaN         NaN\n",
       "3  230000.0         NaN         NaN\n",
       "4  178000.0         NaN         NaN"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output=pd.concat([data_verify,data_predicted],axis=1)\n",
    "final_output['Difference']=final_output['Test']-final_output['Prediction']\n",
    "final_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9237688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Saving The Model\n",
    "#### Save the model\n",
    "torch.save(model,'HousePrice.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ee3c115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'HouseWeights.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "24508e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the saved Model\n",
    "embs_size=[(15, 8), (5, 3), (2, 1), (4, 2)]\n",
    "model1=FeedForwardNN(embs_size,5,1,[100,50],p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "68b240e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FeedForwardNN:\n\tsize mismatch for bn_cont.weight: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([5]).\n\tsize mismatch for bn_cont.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([5]).\n\tsize mismatch for bn_cont.running_mean: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([5]).\n\tsize mismatch for bn_cont.running_var: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([5]).\n\tsize mismatch for layers.0.weight: copying a param with shape torch.Size([100, 18]) from checkpoint, the shape in current model is torch.Size([100, 19]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17512\\2229105223.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HouseWeights.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\envpytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1670\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1672\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FeedForwardNN:\n\tsize mismatch for bn_cont.weight: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([5]).\n\tsize mismatch for bn_cont.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([5]).\n\tsize mismatch for bn_cont.running_mean: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([5]).\n\tsize mismatch for bn_cont.running_var: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([5]).\n\tsize mismatch for layers.0.weight: copying a param with shape torch.Size([100, 18]) from checkpoint, the shape in current model is torch.Size([100, 19])."
     ]
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load('HouseWeights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0fb63ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df20b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
